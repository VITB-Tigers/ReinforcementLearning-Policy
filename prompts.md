# DQN Snake Game Prompts

## Business Problem Statement
How can the Deep Q-Network (DQN) algorithm leverage neural networks to train an intelligent Snake Game agent capable of approximating optimal policies in a dynamic environment?

---

## Prompts

### 1. Basics of DQN
1. How does DQN differ from Q-learning in terms of state-action evaluation?
2. What is the role of the neural network in DQN?
3. Why is a target network necessary in DQN?

### 2. Training Components
4. What is experience replay, and how does it improve the stability of training in DQN?
5. How does the loss function in DQN guide the training process?
6. Why is it important to periodically update the target network during training?

### 3. Hyperparameters
7. How does the batch size used in training impact the performance of DQN?
8. What role do the learning rate and discount factor play in DQN optimization?

### 4. Advanced Optimization
9. How can you prevent overfitting when training a DQN agent for the Snake Game?
10. How does the neural network architecture (e.g., number of layers, activation functions) affect the agent's performance?
11. How would you extend DQN to handle continuous state-action spaces?

### 5. Practical Applications
12. What challenges arise when applying DQN to environments with sparse or misleading rewards?
13. How can visualization of Q-values help debug issues in the DQN agent?
14. How does the training time of DQN compare to simpler approaches like Q-learning or SARSA?
15. What modifications can make DQN more robust in environments with dynamic obstacles?
